######################################## bitwise operation.
----------------------------------------------1st method
def bitwise_operations(a,b): 
    bitwise_and_result = a & b 
    print("a & b =", bitwise_and_result) 
    bitwise_or_result = a | b 
    print("a | b =", bitwise_or_result) 
    bitwise_not_resulta =  ~a 
    print("~a =", bitwise_not_resulta) 
    bitwise_not_resultb = ~b 
    print("~b =", bitwise_not_resultb) 
    bitwise_xor_result = a ^ b 
    print("a ^ b =", bitwise_xor_result) 
    bitwise_rightshift_resulta = a >> 1 
    print("a >> 1 =", bitwise_rightshift_resulta) 
    bitwise_rightshift_resultb = b >> 1 
    print("b >> 1 =", bitwise_rightshift_resultb) 
    bitwise_leftshift_resulta = 1 >> a 
    print("1 >> a =", bitwise_leftshift_resulta) 
    bitwise_leftshift_resultb = 1 >> b 
    print("1 >> b =", bitwise_leftshift_resultb) 
a = int(input("Enter the value of a: ")) 
b = int(input("Enter the value of b: ")) 
bitwise_operations(a,b) 


----------------------------------------------2nd Method 

import pandas as pd 
from sklearn.feature_extraction.text import CountVectorizer 
print('Boolean RetrievalModal Using Bitwise Operations on Term Document Incidence Matrix\n') 
corpus={'this is the first document','this is the second document','and this is the third document','Is this the first document'} 
print("The corupus is: \n",corpus) 
vectorizer = CountVectorizer()
x = vectorizer.fit_transform(corpus) 
df = pd.DataFrame(x.toarray(),columns=vectorizer.get_feature_names()) 
print("\nThe generated data frame\n") 
print(df) 
print("\nQuery processing on Term Document Incidence Matrix") 
#AND 
print("\nFind all document ids for query 'this' AND 'first'") 
alldata = df[(df['this']==1)&(df['first']==1)] 
print("Document ids where with 'this' AND 'first are present are: '", alldata.index.tolist()) 
#OR 
print("\nFind all document for query 'this' OR 'first'") 
alldata = df[(df['this']==1)|(df['first']==1)] 
print("Document ids where eutger 'this' OR 'first are present are: ", alldata.index.tolist()) 
#NOT 
print("\nFind all document for query NOT 'and'") 
alldata = df[(df['and']!=1)] 
print("Document ids where 'and' is not present are:", alldata.index.tolist()) 





######################################PAGE RANK
-----------------------1st Method – Without using NetworkX 

def page_rank(graph, damping_factor=0.85, max_iterations=100, tolerance=1e-6): 
    num_pages = len(graph) 
    initial_page_rank = 1.0 / num_pages 
    page_ranks = {page:initial_page_rank for page in graph} 
    for _ in range(max_iterations): 
        new_page_ranks = {} 
        for page in graph: 
            new_rank = (1-damping_factor)/num_pages 
            for link in graph: 
                if page in graph[link]: 
                    new_rank += damping_factor * (page_ranks[link]/len(graph[link])) 
            new_page_ranks[page] = new_rank 
        convergence = all(abs(new_page_ranks[page] - page_ranks[page]) <  tolerance for page in graph) 
        if convergence: 
            break 
        page_ranks = new_page_ranks 
    return page_ranks 
if __name__ == "__main__": 
    graph = { 
        'A':['B','C'], 
        'B':['A'], 
        'C':['A','B'], 
        'D':['B'] 
    } 
    result=page_rank(graph) 
    for page, rank in sorted(result.items(),key=lambda x: x[1], reverse=True): 
        print(f"Page: {page} - PageRank: {rank:.4f}") 


-----------------------------------2nd Method – Using NetworkX with weighted graph 

import networkx as nx 
import pylab as plt 
G = nx.DiGraph() 
G.add_weighted_edges_from([('A','B',1),('A','C',1),('C','A',1),('B','C',1)]) 
ppr1 = nx.pagerank(G) 
print("Page rank value: ", ppr1) 
pos = nx.spiral_layout(G) 
nx.draw(G, pos, with_labels=True, node_color="#f86e00") 
plt.show() 
 

 

--------------------------------3rd Method – Using NetworkX without weighted graph 

import networkx as nx 
#import matplotlib.pyplot as plt 
import pylab as plt 
G = nx.DiGraph() 
[G.add_node(k) for k in ["A", "B", "C", "D", "E", "F", "G"]] 
G.add_edges_from([('G', 'A'), ('A', 'G'), ('B', 'A'), 
                  ('A', 'C'), ('C', 'A'), ('F', 'A'), 
                  ('E', 'A'), ('A', 'D'), ('D', 'F'), 
                 ('D','B')]) 
ppr1 = nx.pagerank(G) 
print("Page rank value: ", ppr1) 
pos = nx.spiral_layout(G) 
nx.draw(G, pos, with_labels=True, node_color="#f86e00") 
plt.show()


###############################Levenshtein Distance 
  
def leven(x, y): 
    n = len(x) 
    m = len(y) 
    A = [[i+j for j in range(m + 1)]for i in range(n + 1)] 
    for i in range(n): 
        for j in range(m): 
            A[i+1][j+1] = min(A[i][j + 1]+1, 
                             A[i + 1][j]+1, 
                             A[i][j]+int(x[i]!=y[j])) 
    return A[n][m] 
print(leven("brap","rap")) 
print(leven("trial","try")) 
print(leven("horse","force")) 
print(leven("rose","erode")) 

############################JACCARD SIMILARITY


def Jaccard_Similarity(doc1, doc2): 
    words_doc1 = set(doc1.lower().split()) 
    words_doc2 = set(doc2.lower().split()) 
    intersection = words_doc1.intersection(words_doc2) 
    union = words_doc1.union(words_doc2) 
    return float(len(intersection))/len(union) 
doc_1 = "Data is the new oil of the digital economy" 
doc_2 = "Data is a new oil" 
Jaccard_Similarity(doc_1, doc_2) 

 

###############################COSINE SIMILARITY

doc1 = "Data is the new oil of the digital economy"
doc2 = "Data is the new oil"
data = [doc1, doc2]
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
vectorizer = TfidfVectorizer()
vector_matrix = vectorizer.fit_transform(data)
tokens = vectorizer.get_feature_names()
create_dataframe = (vector_matrix.toarray(), tokens)
cosine_similarity_matrix = cosine_similarity(vector_matrix)
create_dataframe = cosine_similarity_matrix[0, 1]
print(create_dataframe)





###################################mapreducer
from functools import reduce 
from collections import defaultdict 
def mapper(data): 
    char_count = defaultdict(int) 
    for char in data: 
        if char.isalpha(): 
            char_count[char.lower()] += 1 
    return char_count.items() 
def reducer(counts1, counts2): 
    merged_counts = defaultdict(int) 
    for char, count in counts1: 
        merged_counts[char] += count 
    for char, count in counts2: 
        merged_counts[char] += count 
    return merged_counts.items() 
if __name__ =="__main__": 
    dataset = "Hello World! This is a MapReduce example." 
    chunks = [chunk for chunk in dataset.split()] 
    mapped_results = map(mapper, chunks) 
    final_counts = reduce(reducer, mapped_results) 
    for char, count in final_counts: 
        print(f"Character: {char}, Count: {count}") 



######################################HITS ALGORITHM
import networkx as nx 
G = nx.DiGraph() 
G.add_edges_from([(1,2),(1,3),(2,4),(3,4),(4,5)]) 
authority_scores, hub_scores = nx.hits(G) 
print("Authority Scores: ",authority_scores) 
print("Hub Scores: ",hub_scores) 




####################### ######STOP WORDS
import nltk 
nltk.download('stopwords') 
from nltk.corpus import stopwords 
set(stopwords.words('english')) 

 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
example_sent = "This is a simple sentence, showing off the stop words filtration." 
stop_words = set(stopwords.words('english')) 
word_tokens = word_tokenize(example_sent) 
filtered_sentence = [w for w in word_tokens if not w in stop_words] 
print(word_tokens) 
print(filtered_sentence) 

#################################TWITTER SCRAPING

  
import pandas as pd 
from ntscraper import Nitter 
scraper = Nitter() 
tweets = scraper.get_tweets('actorvijay', mode='user', number=5) 
tweets 
final_tweets=[] 
for tweet in tweets['tweets']: 
    data = [tweet['link'],tweet['text'],tweet['date'],tweet['stats']['likes']] 
    final_tweets.append(data) 
final_tweets 
data = pd.DataFrame(final_tweets, columns=['link','text','date','stats']) 
data 





###################################WEB CRAWLER
import requests 
from parsel import Selector 
import time 
start = time.time() 
response = requests.get('http://recurship.com/') 
selector = Selector(response.text) 
href_links = selector.xpath('//a/@href').getall() 
image_links = selector.xpath('//img/@src').getall() 
print("******************* Href link *****************************") 
print(href_links) 
print("*******************/href_links*****************************") 
print("****************** Image Link ****************************") 
print(image_links) 
print("*******************/image_links*****************************") 
end=time.time() 
print("Time taken in seconds: ",(end-start)) 




###################################PAGE RANK USING XML RETRIEVAL
import networkx as nx 
import xml.etree.ElementTree as ET 
def parse_xml(xml_text): 
    root = ET.fromstring(xml_text) 
    return root 
def generate_web_graph(xml_root): 
    G = nx.DiGraph() 
    for page in xml_root.findall('.//page'): 
        page_id = page.find('id').text 
        G.add_node(page_id) 
        links = page.findall('.//link') 
        for link in links: 
            target_page_id = link.text 
            G.add_edge(page_id,target_page_id)
    return G 

def compute_topic_specific_pagerank(graph, topic_nodes, alpha=0.85, max_iter = 100, tol = 1e-6): 
    personalization = {node: 1.0 if node in topic_nodes else 0.0 for node in graph.nodes} 
    return nx.pagerank(graph, alpha=alpha, personalization=personalization, max_iter=max_iter, tol=tol) 
if __name__ == "__main__": 
    xml_data = """ 
    <webgraph> 
        <page> 
            <id>1</id> 
            <link>2</link> 
            <link>3</link> 
        </page> 
        <page> 
            <id>2</id> 
            <link>1</link>
            <link>3</link> 
        </page> 
        <page> 
            <id>3</id> 
            <link>1</link> 
            <link>2</link> 
        </page> 
    </webgraph>""" 
    xml_root = parse_xml(xml_data)
    web_graph = generate_web_graph(xml_root) 
    topic_specific_pagerank = compute_topic_specific_pagerank(web_graph, topic_nodes=['1','2']) 
    print("Topic-Specific PageRank") 
    for node, score in sorted(topic_specific_pagerank.items(),key=lambda x:x[1], reverse=True): 
        print(f"Node: {node} - PageRank: {score:4f}") 


####################################XML RETRIEVAL(INCASE ASK DO THIS, IF LIBRARY GIVEN CHECK AND PASTE THE CORRECT CODE)
---METHOD1

import xml.etree.ElementTree as ET 
xml_data = '''<root> 
        <person> 
            <name>WOH</name> 
            <age>30</age> 
            <city>New York</city> 
        </person> 
        <person> 
            <name>Alice</name> 
            <age>25</age> 
            <city>London</city> 
        </person> 
    </root>''' 
tree = ET.fromstring(xml_data) 
for person in tree.findall('person'): 
    name = person.find('name').text 
    age = person.find('age').text 
    city = person.find('city').text 
    print(f"Name: {name}, Age: {age}, City: {city}") 



------METHOD2

from lxml import etree 
xml_data = '''<root> 
        <person> 
            <name>WOH</name> 
            <age>30</age> 
            <city>New York</city> 
        </person> 
        <person> 
            <name>Alice</name> 
            <age>25</age> 
            <city>London</city> 
        </person> 
    </root>''' 
tree = etree.fromstring(xml_data) 
for person in tree.xpath('//person'): 
    name = person.xpath('name/text()')[0] 
    age = person.xpath('age/text()')[0] 
    city = person.xpath('city/text()')[0] 
    print(f"Name: {name}, Age: {age}, City: {city}")









































